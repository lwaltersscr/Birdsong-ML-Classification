{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "CU0KKsbxvr4a"
   },
   "source": [
    "# Data Mining Final Project\n",
    "## Identifying Bird Species from Audio Spectrograms using ML\n",
    "\n",
    "text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of classes: 20\n",
      "Epoch 1/20\n",
      "  Train Loss: 2.9975 | Train Acc: 0.0638\n",
      "  Val   Loss: 2.9911 | Val   Acc: 0.0932\n",
      "Epoch 2/20\n",
      "  Train Loss: 2.9649 | Train Acc: 0.0845\n",
      "  Val   Loss: 2.9299 | Val   Acc: 0.0980\n",
      "Epoch 3/20\n",
      "  Train Loss: 2.8603 | Train Acc: 0.1180\n",
      "  Val   Loss: 2.8058 | Val   Acc: 0.1366\n",
      "Epoch 4/20\n",
      "  Train Loss: 2.7653 | Train Acc: 0.1592\n",
      "  Val   Loss: 2.7322 | Val   Acc: 0.1850\n",
      "Epoch 5/20\n",
      "  Train Loss: 2.6857 | Train Acc: 0.1896\n",
      "  Val   Loss: 2.6770 | Val   Acc: 0.1905\n",
      "Epoch 6/20\n",
      "  Train Loss: 2.6076 | Train Acc: 0.2170\n",
      "  Val   Loss: 2.5737 | Val   Acc: 0.2243\n",
      "Epoch 7/20\n",
      "  Train Loss: 2.5261 | Train Acc: 0.2455\n",
      "  Val   Loss: 2.5306 | Val   Acc: 0.2077\n",
      "Epoch 8/20\n",
      "  Train Loss: 2.4456 | Train Acc: 0.2697\n",
      "  Val   Loss: 2.4311 | Val   Acc: 0.2767\n",
      "Epoch 9/20\n",
      "  Train Loss: 2.4049 | Train Acc: 0.2817\n",
      "  Val   Loss: 2.4144 | Val   Acc: 0.2719\n",
      "Epoch 10/20\n",
      "  Train Loss: 2.3423 | Train Acc: 0.3109\n",
      "  Val   Loss: 2.3326 | Val   Acc: 0.3347\n"
     ]
    }
   ],
   "source": [
    "# ============================================\n",
    "# Imports\n",
    "# ============================================\n",
    "import os\n",
    "import random\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torchaudio\n",
    "import librosa\n",
    "\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# ============================================\n",
    "# Config\n",
    "# ============================================\n",
    "SEED = 42\n",
    "random.seed(SEED)\n",
    "np.random.seed(SEED)\n",
    "torch.manual_seed(SEED)\n",
    "\n",
    "DEVICE = \"mps\" if torch.backends.mps.is_available() else \"cpu\"\n",
    "\n",
    "METADATA_PATH = \"train_metadata.csv\"\n",
    "TRAIN_AUDIO_DIR = \"train_audio\"\n",
    "\n",
    "SAMPLE_RATE = 32000\n",
    "CLIP_SECONDS = 5\n",
    "NUM_SAMPLES = SAMPLE_RATE * CLIP_SECONDS\n",
    "N_MELS = 64\n",
    "\n",
    "BATCH_SIZE = 32\n",
    "NUM_EPOCHS = 20\n",
    "LEARNING_RATE = 1e-3\n",
    "TOP_K = 20\n",
    "\n",
    "# ============================================\n",
    "# Audio helpers\n",
    "# ============================================\n",
    "def load_audio(path):\n",
    "    audio, sr = librosa.load(path, sr=SAMPLE_RATE, mono=True)\n",
    "    return audio, sr\n",
    "\n",
    "def random_crop_or_pad(audio):\n",
    "    if len(audio) > NUM_SAMPLES:\n",
    "        start = random.randint(0, len(audio) - NUM_SAMPLES)\n",
    "        audio = audio[start:start + NUM_SAMPLES]\n",
    "    elif len(audio) < NUM_SAMPLES:\n",
    "        pad = NUM_SAMPLES - len(audio)\n",
    "        audio = np.pad(audio, (0, pad))\n",
    "    return audio\n",
    "\n",
    "def audio_to_mel_spectrogram(audio):\n",
    "    mel = librosa.feature.melspectrogram(\n",
    "        y=audio,\n",
    "        sr=SAMPLE_RATE,\n",
    "        n_mels=N_MELS,\n",
    "        n_fft=2048,\n",
    "        hop_length=512\n",
    "    )\n",
    "    mel = librosa.power_to_db(mel, ref=np.max)\n",
    "    return mel\n",
    "\n",
    "# ============================================\n",
    "# Load + filter metadata\n",
    "# ============================================\n",
    "metadata = pd.read_csv(METADATA_PATH)\n",
    "metadata = metadata.dropna(subset=[\"primary_label\", \"filename\"])\n",
    "\n",
    "species_counts = metadata[\"primary_label\"].value_counts()\n",
    "top_species = species_counts.head(TOP_K).index\n",
    "\n",
    "metadata = metadata[metadata[\"primary_label\"].isin(top_species)]\n",
    "metadata = metadata.reset_index(drop=True)\n",
    "\n",
    "# ============================================\n",
    "# Label mapping (AFTER filtering)\n",
    "# ============================================\n",
    "unique_labels = sorted(metadata[\"primary_label\"].unique())\n",
    "label_to_index = {label: i for i, label in enumerate(unique_labels)}\n",
    "index_to_label = {i: label for label, i in label_to_index.items()}\n",
    "\n",
    "metadata[\"label_idx\"] = metadata[\"primary_label\"].map(label_to_index)\n",
    "NUM_CLASSES = len(unique_labels)\n",
    "\n",
    "print(\"Number of classes:\", NUM_CLASSES)\n",
    "\n",
    "# ============================================\n",
    "# Train / validation split\n",
    "# ============================================\n",
    "train_df, val_df = train_test_split(\n",
    "    metadata,\n",
    "    test_size=0.2,\n",
    "    stratify=metadata[\"label_idx\"],\n",
    "    random_state=SEED\n",
    ")\n",
    "\n",
    "train_df = train_df.reset_index(drop=True)\n",
    "val_df = val_df.reset_index(drop=True)\n",
    "\n",
    "# ============================================\n",
    "# Dataset\n",
    "# ============================================\n",
    "class BirdClefDataset(Dataset):\n",
    "    def __init__(self, df, audio_dir, training=True):\n",
    "        self.df = df\n",
    "        self.audio_dir = audio_dir\n",
    "        self.training = training\n",
    "\n",
    "        self.freq_mask = torchaudio.transforms.FrequencyMasking(freq_mask_param=8)\n",
    "        self.time_mask = torchaudio.transforms.TimeMasking(time_mask_param=15)\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.df)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        row = self.df.iloc[idx]\n",
    "        path = os.path.join(self.audio_dir, row[\"filename\"])\n",
    "        label = row[\"label_idx\"]\n",
    "\n",
    "        audio, _ = load_audio(path)\n",
    "        audio = random_crop_or_pad(audio)\n",
    "        mel = audio_to_mel_spectrogram(audio)\n",
    "\n",
    "        mel = (mel - mel.mean()) / (mel.std() + 1e-6)\n",
    "        mel = torch.tensor(mel, dtype=torch.float32).unsqueeze(0)\n",
    "\n",
    "        if self.training:\n",
    "            if torch.rand(1).item() < 0.5:\n",
    "                mel = self.freq_mask(mel)\n",
    "            if torch.rand(1).item() < 0.5:\n",
    "                mel = self.time_mask(mel)\n",
    "\n",
    "        return mel, torch.tensor(label)\n",
    "\n",
    "# ============================================\n",
    "# DataLoaders (macOS safe)\n",
    "# ============================================\n",
    "train_dataset = BirdClefDataset(train_df, TRAIN_AUDIO_DIR, training=True)\n",
    "val_dataset = BirdClefDataset(val_df, TRAIN_AUDIO_DIR, training=False)\n",
    "\n",
    "train_loader = DataLoader(train_dataset, batch_size=BATCH_SIZE, shuffle=True, num_workers=0)\n",
    "val_loader = DataLoader(val_dataset, batch_size=BATCH_SIZE, shuffle=False, num_workers=0)\n",
    "\n",
    "# ============================================\n",
    "# Simple CNN\n",
    "# ============================================\n",
    "class SimpleBirdCNN(nn.Module):\n",
    "    def __init__(self, num_classes):\n",
    "        super().__init__()\n",
    "        self.conv = nn.Sequential(\n",
    "            nn.Conv2d(1, 16, 3, padding=1),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(2),\n",
    "\n",
    "            nn.Conv2d(16, 32, 3, padding=1),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(2),\n",
    "\n",
    "            nn.Conv2d(32, 64, 3, padding=1),\n",
    "            nn.ReLU(),\n",
    "            nn.AdaptiveAvgPool2d((1, 1))\n",
    "        )\n",
    "\n",
    "        self.fc = nn.Linear(64, num_classes)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.conv(x)\n",
    "        x = x.view(x.size(0), -1)\n",
    "        return self.fc(x)\n",
    "\n",
    "model = SimpleBirdCNN(NUM_CLASSES).to(DEVICE)\n",
    "\n",
    "# ============================================\n",
    "# Loss + optimizer (fixed class weights)\n",
    "# ============================================\n",
    "class_counts = train_df[\"label_idx\"].value_counts().sort_index()\n",
    "weights = 1.0 / class_counts.values\n",
    "weights = weights * (len(weights) / weights.sum())\n",
    "weights = torch.tensor(weights, dtype=torch.float32).to(DEVICE)\n",
    "\n",
    "criterion = nn.CrossEntropyLoss(weight=weights)\n",
    "optimizer = optim.Adam(model.parameters(), lr=LEARNING_RATE)\n",
    "\n",
    "# ============================================\n",
    "# Training loops\n",
    "# ============================================\n",
    "def train_one_epoch(model, loader):\n",
    "    model.train()\n",
    "    total_loss, correct, total = 0, 0, 0\n",
    "\n",
    "    for x, y in loader:\n",
    "        x, y = x.to(DEVICE), y.to(DEVICE)\n",
    "        optimizer.zero_grad()\n",
    "        out = model(x)\n",
    "        loss = criterion(out, y)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        total_loss += loss.item()\n",
    "        preds = out.argmax(1)\n",
    "        correct += (preds == y).sum().item()\n",
    "        total += y.size(0)\n",
    "\n",
    "    return total_loss / len(loader), correct / total\n",
    "\n",
    "def validate_one_epoch(model, loader):\n",
    "    model.eval()\n",
    "    total_loss, correct, total = 0, 0, 0\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for x, y in loader:\n",
    "            x, y = x.to(DEVICE), y.to(DEVICE)\n",
    "            out = model(x)\n",
    "            loss = criterion(out, y)\n",
    "\n",
    "            total_loss += loss.item()\n",
    "            preds = out.argmax(1)\n",
    "            correct += (preds == y).sum().item()\n",
    "            total += y.size(0)\n",
    "\n",
    "    return total_loss / len(loader), correct / total\n",
    "\n",
    "# ============================================\n",
    "# Train\n",
    "# ============================================\n",
    "for epoch in range(NUM_EPOCHS):\n",
    "    tr_loss, tr_acc = train_one_epoch(model, train_loader)\n",
    "    val_loss, val_acc = validate_one_epoch(model, val_loader)\n",
    "\n",
    "    print(f\"Epoch {epoch+1}/{NUM_EPOCHS}\")\n",
    "    print(f\"  Train Loss: {tr_loss:.4f} | Train Acc: {tr_acc:.4f}\")\n",
    "    print(f\"  Val   Loss: {val_loss:.4f} | Val   Acc: {val_acc:.4f}\")\n",
    "\n",
    "# ============================================\n",
    "# Inference on a single audio file\n",
    "# ============================================\n",
    "def predict_audio(path, top_k=5):\n",
    "    model.eval()\n",
    "    audio, _ = load_audio(path)\n",
    "    audio = random_crop_or_pad(audio)\n",
    "    mel = audio_to_mel_spectrogram(audio)\n",
    "    mel = (mel - mel.mean()) / (mel.std() + 1e-6)\n",
    "    mel = torch.tensor(mel, dtype=torch.float32).unsqueeze(0).unsqueeze(0).to(DEVICE)\n",
    "\n",
    "    with torch.no_grad():\n",
    "        out = model(mel)\n",
    "        probs = torch.softmax(out, dim=1)\n",
    "\n",
    "    top_probs, top_idxs = torch.topk(probs, top_k)\n",
    "\n",
    "    for p, i in zip(top_probs[0], top_idxs[0]):\n",
    "        print(index_to_label[i.item()], f\"{p.item():.3f}\")"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "name": "UN_8-3.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
